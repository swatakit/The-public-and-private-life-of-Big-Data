{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Verdana, cursive, sans-serif\" >\n",
    "<h1><center>Webpage Scrapping with Python</center></h1>\n",
    "\n",
    "<b>Author: Siraprapa Watakit</b>\n",
    "<br><b>Last edited: November 2018</b>\n",
    "<p><b>Synopsis:</b></p>\n",
    "\n",
    "<p>This python notebook is a part of a working paper <b>\"The Public and Private Life of Big Data\",Pavabutr and Watakit(2018)</b>. This notebook demonstrates a step-by-step data scrapping from Reuters company news. </p>\n",
    "\n",
    "<p>The following libraries are required for data scrapping project</p>\n",
    "<ul>\n",
    "   <li><code>panda,numpy</code>&nbsp; For dataframe, vector and matric processing</li>\n",
    "   <li><code>BeautifulSoup</code> &nbsp;  For HTML data parsing</li>\n",
    "   <li><code>urllib.request</code> &nbsp; For HTML pages requesting </li>\n",
    "   <li><code>os,time,datetime</code> &nbsp;   Miscellaneous helper libraries for filepath, date and datetime manipulation</li>\n",
    "</ul>\n",
    "\n",
    "<p>Note that if you have installed Anaconda 3, the above libraries would have already been installed. Otherwise, please refer to <code>pip</code> installation manual</p>\n",
    "\n",
    "<p><b>Pre-requisite knowledge and self-paced learning references:</b></p>\n",
    "<p>It is recommended that the reader have the basic knowledge of the Pyhton/HTML/CSS and BeautifulSoup. Free learning materials are provided as follows:\n",
    "<ul>\n",
    "    <li> <a href=\"https://www.youtube.com/watch?v=YYXdXT2l-Gg&list=PL-osiE80TeTt2d9bfVyTiXJA-UTHn6WwU\">Python Tutorial for Beginners</a>  </li>\n",
    "    <li> <a href=\"https://www.youtube.com/watch?v=UB1O30fR-EE&list=PLillGF-RfqbZTASqIqdvm1R5mLrQq79CU&index=1\">HTML and CSS for Beginners</a>  </li>\n",
    "    <li> <a href=\"https://www.youtube.com/watch?v=XQgXKtPSzUI\">Web Scraping with Python and Beautiful Soup</a>  </li>\n",
    "</ul>\n",
    "\n",
    "<p><b>Disclaimer:</b>This material is for informational and educational purposes only. </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Verdana, cursive, sans-serif\" >\n",
    "<h2><center>End-to-End Process of Web Scrapping</center></h2>\n",
    "<p>Company news are extracted from Reuters News Website. By specifying a ticket and a date, we can retrieve news of the stock at the specific date. A webpage is a collection of HTML text-based document, in which  contents and styles of a webpage is structured and controlled by HTML elements. We will use BeautifulSoup to extract specific news contents from specific HTML elements</p>\n",
    "\n",
    "<img src=\"./images/rtnews.png\" >\n",
    "\n",
    "<p>In this program, we will programmatically retrieve and parse Thailand SET50 stocks during a specific period. Note that the ticker lists and period is configurable.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Verdana, cursive, sans-serif\">\n",
    "<b>Step 1 Import libraries</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen \n",
    "import datetime\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Verdana, cursive, sans-serif\">\n",
    "<b>Step 2 Define utility functions</b>\n",
    "<ul>\n",
    "    <li><code>gen_daterange()</code> this function generate a sequence of daily datetime</li>\n",
    "    <li><code>parse_rtnews()</code> this function request an HTML page, given an input of ticker and date. Upon receiving an HTML, it will parse the HTML content and extract only the news contents that is wrapped with HTML elements:<b>div</b> <b>class:</b><code>topStory</code>. Not that this function is a specific parser for Reuters news webpage.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen_daterange(year,month,day,numdays):\n",
    "    \"\"\"Generate N days until now, e.g., [20151231, 20151230,..].\"\"\"\n",
    "    base = datetime.datetime(year,month,day)\n",
    "    date_range = [base - datetime.timedelta(days=x) for x in range(0, numdays)]\n",
    "    return [x.strftime(\"%Y%m%d\") for x in date_range]\n",
    "\n",
    "failed_tickers=[]\n",
    "def parse_rtnews(url,ticker,date):\n",
    "    \"\"\"Parse topstory company news by tiecker and date\"\"\"\n",
    "    #initialize\n",
    "    ticker_list=[]\n",
    "    date_list=[]\n",
    "    title_list=[]\n",
    "    body_list=[]\n",
    "    url_ticker=url + ticker +\"?date=\"+date\n",
    "    \n",
    "    #there maybe some hickups while open url, if so retry a few times, else just skip this one\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            url_client = urlopen(url_ticker)\n",
    "            html = url_client.read()\n",
    "            url_client.close()\n",
    "            soupHtml = soup(html,\"html.parser\") \n",
    "            content=soupHtml.find_all(\"div\", {'class': 'topStory'})\n",
    "            if [ticker,date] in failed_tickers: failed_tickers.remove([ticker,date])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            if not [ticker,date] in failed_tickers: failed_tickers.append([ticker,date])\n",
    "            print(e)\n",
    "            print('retry...')\n",
    "            time.sleep(np.random.poisson(2))\n",
    "            continue\n",
    "    \n",
    "    #if there is news, let's parse it, if not, the function will return empty lists\n",
    "    if len(content):\n",
    "        print(\"Parsing company news: {}, {}\".format(ticker,date))\n",
    "        for i in range(len(content)):\n",
    "            ticker_list.append(ticker)\n",
    "            date_list.append(date)\n",
    "            title = content[i].h2.get_text().replace(\",\", \" \").replace(\"\\n\", \" \")\n",
    "            title_list.append(title)\n",
    "            body = content[i].p.get_text().replace(\",\", \" \").replace(\"\\n\", \" \")\n",
    "            body_list.append(body)    \n",
    "    \n",
    "    return ticker_list,date_list,title_list,body_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Verdana, cursive, sans-serif\">\n",
    "<b>Step 3 Configure the tickers  and dates</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = pd.read_csv(\"./input/tickersSET.csv\")\n",
    "tickers_list=tickers[\"TICKER\"].tolist()\n",
    "url = \"https://www.reuters.com/finance/stocks/company-news/\"\n",
    "outfile=\"./output/rtnews/rtnews_\"\n",
    "daterange=gen_daterange(2017,12,31,numdays=365) #from this yyyy/mm/dd, crawl back numsday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Verdana, cursive, sans-serif\">\n",
    "<b>Step 4 Looping through tickers and dates</b>\n",
    "<p>The output from this step are csv files by dates.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for date in daterange:\n",
    "    #initialize\n",
    "    ticker_list_global=[]\n",
    "    date_list_global=[]\n",
    "    title_list_global=[]\n",
    "    body_list_global=[]\n",
    "    print(\"Crawling news date:{}\".format(date))\n",
    "    for ticker in tickers_list:\n",
    "        print(\"\\t\" + ticker)\n",
    "        datetimeobject = datetime.datetime.strptime(date,'%Y%m%d')\n",
    "        newformat = datetimeobject.strftime('%m%d%Y')\n",
    "        ticker_list,date_list,title_list,body_list=parse_rtnews(url,ticker,newformat)        \n",
    "        if len(ticker_list):\n",
    "            ticker_list_global.extend(ticker_list)\n",
    "            date_list_global.extend(date_list)\n",
    "            title_list_global.extend(title_list)\n",
    "            body_list_global.extend(body_list)\n",
    "\n",
    "    #if has news, save file by date\n",
    "    if len(ticker_list_global)>0:\n",
    "        print(\"Saving : {}\".format(outfile+date+\".csv\"))\n",
    "        headers=[\"ticker\",\"date\",\"title\",\"body\"]\n",
    "        data=pd.DataFrame([ticker_list_global,date_list_global,title_list_global,body_list_global])\n",
    "        data=data.transpose()\n",
    "        data.to_csv(outfile+date+\".csv\",encoding='utf-8-sig',index=False,header=headers,sep=\"|\")\n",
    "\n",
    "#%% Checking - is there any failed attempts?, if there is, we may want to rerun some dates/tickers later\n",
    "if len(failed_tickers):\n",
    "    print(\"There are some failed attempts\")\n",
    "    print(failed_tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Verdana, cursive, sans-serif\">\n",
    "<b>Step 5 Cleanup and aggregate all csv files</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile_path=\"./output/rtnews/\"\n",
    "filename_final_csv=outfile_path+\"_rtnews_all_.csv\"\n",
    "files = [file for file in os.listdir(outfile_path) if file.endswith(\".csv\")]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for file in files:\n",
    "    tmp = pd.read_csv(outfile_path + file, sep='|',encoding='utf-8-sig')\n",
    "    df = df.append(tmp, ignore_index=True)\n",
    "\n",
    "#%% save the concating result to one file\n",
    "df=df.drop_duplicates(subset=['ticker','title','body'],keep='last')\n",
    "df.to_csv(filename_final_csv,sep='|',index=False,encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
